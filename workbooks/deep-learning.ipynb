{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducton to Deep Learning\n",
    "\n",
    "\n",
    "Here we will look at how to build a very basic neural network. \n",
    "\n",
    "Probably the most basic (and pretty much completely useless) neural network possible follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 0.1\n",
    "\n",
    "def neural_network(input, weight):\n",
    "    return input * weight\n",
    "\n",
    "some_values = [8.5, 9.5, 10, 9]\n",
    "input = some_values[0]\n",
    "\n",
    "prediction = neural_network(input, weight)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are saying that, based on the given weight, we will derive a prediction for the given input. Calling this a neural network is a bit of a stretch but it's a step on the way to understanding how they work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalty Score Problem\n",
    "\n",
    "We will build a neural network to predict the probability of a soccer player with particular attributes, scoring a penalty against a goal keeper with particular attributes. \n",
    "\n",
    "*Disclaimer: I made all the following data up and know nothing about soccer so don't take the specifics of the data seriously.*\n",
    "\n",
    "Imagine we have a list of players and a list of goal keepers. We have some data about player attributes (only power, accuracy and composure). We have some data about keeper attributes (agility, speed). We want to predict the probability of a player scoring against a keeper based on these attributes. \n",
    "\n",
    "What we want to do is feed the attributes of the player and keeper into the neural net with a weight for each attribute. Then we multiply each attribute by its respective weight. We sum all those results together and arrive at a probability (the prediction). \n",
    "\n",
    "* How do we know if the prediction is correct? We would usually have a test set of data to compare against. \n",
    "* What do we change to arrive at the correct result? The weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, power, accuracy, composure):\n",
    "        self.power = power\n",
    "        self.accuracy = accuracy\n",
    "        self.composure = composure\n",
    "        \n",
    "class Keeper:\n",
    "    def __init__(self, agility, speed):\n",
    "        self.agility = agility\n",
    "        self.speed = speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    [Player(0.8, 0.9, 0.3), Keeper(0.7, 0.9), 0.7],\n",
    "    [Player(0.7, 0.6, 0.9), Keeper(0.5, 0.6), 0.74],\n",
    "    [Player(0.8, 0.2, 0.8), Keeper(0.8, 0.9), 0.71],\n",
    "    [Player(0.7, 0.7, 0.5), Keeper(0.8, 0.6), 0.65]\n",
    "]\n",
    "\n",
    "def build_input_data_from_training_data(training_data): \n",
    "    return list(map(lambda entry: [\n",
    "        entry[0].power,\n",
    "        entry[0].accuracy,\n",
    "        entry[0].composure,\n",
    "        entry[1].agility,\n",
    "        entry[1].speed\n",
    "    ], training_data))\n",
    "\n",
    "input_data = build_input_data_from_training_data(training_data)\n",
    "expected_outputs = list(map(lambda entry: entry[2], training_data))\n",
    "\n",
    "\n",
    "print('Input Data: ', input_data)\n",
    "print('Expected Outputs:', expected_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup Exercise\n",
    "\n",
    "Fill in the missing code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(a, b) :\n",
    "    \"\"\" Derive weighted sum of 2 vectors.\n",
    "        \n",
    "        a -- left vector\n",
    "        y -- right vector\n",
    "    \"\"\"\n",
    "    assert(len(a) == len(b))\n",
    "    output = 0\n",
    "    for i in range(len(a)) :\n",
    "        # What goes here?\n",
    "        output += a[i] * b[i]\n",
    "    return round(output, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the neural network function.\n",
    "\n",
    "*Hint: it can be done in one line.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(inputs, weights):\n",
    "    # Implement me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out the weights\n",
    "\n",
    "We have training data and a neural network.\n",
    "\n",
    "Now we need the knowledge for the neural network to make predictions. Kowledge in this context is the __weights__. By tweaking the weights, we can teach this network to make accurate predictions based on what we know.\n",
    "\n",
    "All the weights below are initilaized to 0, except `power_weight` which has been given a value to help you get started.\n",
    "\n",
    "Tweak the weights until your predictions match the expected predictions. \n",
    "\n",
    "Feel free to change `power_weight` if you like but it's not required. \n",
    "\n",
    "*Hint: all weights should be a value between 0 and 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_weight = 0.15\n",
    "accuracy_weight = 0\n",
    "composure_weight = 0\n",
    "agility_weight = 0\n",
    "speed_weight = 0\n",
    "\n",
    "weights = [power_weight, accuracy_weight, composure_weight, agility_weight, speed_weight]\n",
    "predictions = []\n",
    "\n",
    "for i in range(len(input_data)):\n",
    "    predictions.append(neural_network(input_data[i], weights))\n",
    "\n",
    "# We want predictions and expected predictions to be the same\n",
    "print('Expected Predictions:', expected_outputs)\n",
    "print('Predictions:', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if(predictions[i] == expected_outputs[i]):\n",
    "        accuracy += 25\n",
    "        \n",
    "print('Accuracy:', accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Accuracy: 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter Numpy\n",
    "\n",
    "In machine learning we work primarily with vectors and lists of vectors (Matrices). Numpy is a library that makes working with these data structures in python, way easier. \n",
    "\n",
    "Here's an example converting the above code to use numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def neural_network(inputs, weights):\n",
    "    prediction = inputs.dot(weights)\n",
    "    return prediction\n",
    "\n",
    "weights = np.array(weights)\n",
    "predictions = []\n",
    "\n",
    "for i in range(len(input_data)):\n",
    "    predictions.append(round(neural_network(np.array(input_data[i]), weights), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Expected Predictions:', expected_outputs)\n",
    "print('Predictions:', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember:\n",
    "* Predict\n",
    "* Compare \n",
    "* Learn\n",
    "\n",
    "In the last section we did the predict part.\n",
    "\n",
    "In the next section we will look at the compare and touch on the learn part.\n",
    "\n",
    "What we will do now is a basic version of [__gradient decent__](https://en.wikipedia.org/wiki/Gradient_descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are trying to figure out how wrong a prediction was, we are basically figuring out the maginitude of error.\n",
    "\n",
    "The smaller the error, the less we worry about it (the less correction we do), the larder the error, the bigger the correction.\n",
    "\n",
    "We square the error to ensure a positive number and to magnify large errors. \n",
    "\n",
    "To avoid overcorrecting, we use an alpha value which is a fraction to multiply our weight delta and keep the adjustment small.\n",
    "\n",
    "Figuring out good values for alpha among other variables is part of the art of machine learning optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Decent Exercise\n",
    "\n",
    "Below we take the neural network we built before and instead of trying to determine the weights ourselves, we use gradient decent to figure out the weights for us. \n",
    "\n",
    "I have left out the values of alpha and the number of iterations. Can you figure out the most efficient value for alpha and the corresponding mumber of iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = [0, 0, 0, 0, 0]\n",
    "\n",
    "def neural_network(inputs, weights):\n",
    "    prediction = weighted_sum(inputs, weights)\n",
    "    return prediction\n",
    "    \n",
    "def outer_prod(vec_a, delta):\n",
    "    out = [0] * len(vec_a)\n",
    "    for i in range(len(vec_a)):\n",
    "        out[i] = vec_a[i]*delta\n",
    "    return out\n",
    "\n",
    "    \n",
    "alpha = None # alpha is used to stop overcorrecting the weight\n",
    "goal = [expected_outputs[0]]\n",
    "error = [0]\n",
    "delta = [0]\n",
    "\n",
    "iterations = None # At some point, more interations have no effect and degrade performance\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    prediction = neural_network(input_data[0], weights)\n",
    "    \n",
    "    for i in range(len(goal)):\n",
    "        error[i] = (prediction- goal[i]) ** 2\n",
    "        delta = prediction - goal[i]\n",
    "\n",
    "    weight_deltas = outer_prod(input_data[0], delta)\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= alpha * weight_deltas[i]    \n",
    "\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
